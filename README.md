# ğŸ¢ PrÃ©diction de Consommation Ã‰nergÃ©tique et Ã‰missions CO2 des BÃ¢timents

> **Projet 3 - Parcours Data Scientist - OpenClassrooms**  
> ModÃ©lisation supervisÃ©e pour la prÃ©diction de la consommation Ã©nergÃ©tique et des Ã©missions de gaz Ã  effet de serre des bÃ¢timents non-rÃ©sidentiels de Seattle (2016).

---

## ğŸš€ Installation et Lancement

### PrÃ©requis

- Python 3.12 ou supÃ©rieur
- Git

### Ã‰tapes d'installation

```bash
# 1. Cloner le dÃ©pÃ´t
git clone <URL_DU_REPO>
cd "Projet 3"

# 2. CrÃ©er un environnement virtuel
python3 -m venv env

# 3. Activer l'environnement
source env/bin/activate  # Linux/macOS
# OU
env\Scripts\activate     # Windows

# 4. Mettre Ã  jour pip
pip install --upgrade pip

# 5. Installer les dÃ©pendances
pip install -r requirements.txt

# 6. VÃ©rifier l'installation
pip list
```

### Lancer le notebook

```bash
# Ouvrir avec VS Code
code P3_template_modelistation_supervisee_data_scientist.ipynb

# OU avec Jupyter Notebook
jupyter notebook P3_template_modelistation_supervisee_data_scientist.ipynb
```

**âš ï¸ Important :** SÃ©lectionnez le kernel Python de l'environnement virtuel (`env/bin/python`) dans VS Code.

---

## ğŸ“Š Description du Projet

### Objectifs

Construire **deux modÃ¨les de rÃ©gression supervisÃ©e** pour prÃ©dire :

1. **La consommation Ã©nergÃ©tique** (`SiteEnergyUse(kBtu)`)
2. **Les Ã©missions de CO2** (`TotalGHGEmissions`)

des bÃ¢timents non-rÃ©sidentiels de Seattle, Ã  partir de leurs caractÃ©ristiques structurelles et d'usage.

### Dataset

- **Source** : [2016 Building Energy Benchmarking](https://data.seattle.gov/dataset/2016-Building-Energy-Benchmarking/2bpz-gwpy)
- **Taille initiale** : 3 376 bÃ¢timents Ã— 46 colonnes
- **Taille finale** : 1 649 bÃ¢timents Ã— 40 colonnes (aprÃ¨s nettoyage)
- **PÃ©riode** : AnnÃ©e 2016

### Variables clÃ©s

**Features principales :**
- `PropertyGFATotal` : Surface totale du bÃ¢timent (ftÂ²)
- `LargestPropertyUseTypeGFA` : Surface de l'usage principal
- `PrimaryPropertyType` : Type de bÃ¢timent (HÃ´pital, Bureau, etc.)
- `YearBuilt` : AnnÃ©e de construction
- `NumberofBuildings` : Nombre de bÃ¢timents dans le complexe
- `NumberofFloors` : Nombre d'Ã©tages

**Variables crÃ©Ã©es (Feature Engineering) :**
- `BuildingAge` : Ã‚ge du bÃ¢timent
- `EnergyPerSurface` : Ratio Ã©nergie/surface (prÃ©dicteur #1 pour CO2)
- `SurfaceGasInteraction` : Interaction surface Ã— gaz naturel
- `HasNaturalGas`, `HasElectricity`, `HasSteam` : Indicateurs sources d'Ã©nergie
- `DistanceToCenter` : Distance au centre-ville (Haversine)

---

## ğŸ¯ RÃ©sultats Principaux

### Target 1 - Consommation Ã‰nergÃ©tique

| MÃ©trique | Valeur |
|----------|---------|
| **Meilleur modÃ¨le** | Random Forest (Bagging) |
| **RÂ² Test** | **0.7479** (75% variance expliquÃ©e) |
| **RÂ² Train** | 0.9617 |
| **Overfitting** | 21.4% (acceptable pour RF) |
| **Top Features** | LargestPropertyUseTypeGFA (17%), PropertyGFATotal (14%) |

**ğŸ’¡ Pourquoi Random Forest gagne ?**  
Plus robuste aux outliers grÃ¢ce au **bagging** (parallÃ©lisation de 100 arbres).

### Target 2 - Ã‰missions de CO2

| MÃ©trique | Valeur |
|----------|---------|
| **Meilleur modÃ¨le** | LightGBM (Boosting) ğŸš€ |
| **RÂ² Test** | **0.8967** (90% variance expliquÃ©e) |
| **RÂ² Train** | 0.9805 |
| **Overfitting** | 9.4% (excellent !) |
| **Top Features** | EnergyPerSurface (17.5%), SurfaceGasInteraction (13.9%) |

**ğŸ’¡ Pourquoi LightGBM gagne ?**  
Capte mieux les **interactions complexes** entre features grÃ¢ce au **boosting** sÃ©quentiel.

---

## ğŸ› ï¸ MÃ©thodologie

### 1. Analyse Exploratoire (EDA)

- Nettoyage des donnÃ©es (suppression Multifamily, valeurs nÃ©gatives)
- Analyse des distributions (transformation log nÃ©cessaire)
- Matrice de corrÃ©lation (9 variables clÃ©s)
- Visualisations interactives (Plotly)

### 2. Feature Engineering

- CrÃ©ation de 10 nouvelles features
- Gestion du data leakage (exclusion EnergyPerSurface pour Target 1)
- Encodage OneHot des variables catÃ©gorielles
- Imputation des valeurs manquantes

### 3. ModÃ©lisation

**Algorithmes testÃ©s (4) :**
1. Linear Regression (baseline)
2. Random Forest (bagging) â­
3. Support Vector Regressor (SVR)
4. LightGBM (boosting) â­

**Optimisation :**
- GridSearchCV avec 216 combinaisons (Target 1) et 24 combinaisons (Target 2)
- **Conclusion** : Les modÃ¨les par dÃ©faut surpassent les versions optimisÃ©es !

### 4. Ã‰valuation

- **MÃ©triques** : RÂ², RMSE, MAE
- **Validation** : Train/Test split (80/20)
- **Feature Importance** : Permutation importance
- **Analyse overfitting** : Ã‰cart Train/Test

---

## ğŸ“ˆ Enseignements ClÃ©s

1. **Les features d'interaction sont CRUCIALES**  
   `EnergyPerSurface` + `SurfaceGasInteraction` = 31.42% de l'importance pour CO2

2. **Bagging vs Boosting - Le contexte compte**  
   - **Random Forest** : Meilleur sur donnÃ©es avec outliers (Target 1)
   - **LightGBM** : Meilleur sur donnÃ©es avec interactions complexes (Target 2)

3. **La transformation log est essentielle**  
   Distribution asymÃ©trique â†’ log1p(target) amÃ©liore drastiquement les performances

4. **GridSearchCV n'amÃ©liore pas toujours**  
   ModÃ¨les par dÃ©faut souvent meilleurs avec de bonnes features (-3% et -11.8%)

5. **Les variables GFA (surface par usage) sont prÃ©dictives**  
   `LargestPropertyUseTypeGFA` est le 2Ã¨me prÃ©dicteur le plus fort (+0.846 corrÃ©lation)

---

## ğŸ“ Structure du Projet

```
Projet 3/
â”œâ”€â”€ README.md                                    # Ce fichier
â”œâ”€â”€ requirements.txt                             # DÃ©pendances Python
â”œâ”€â”€ P3_template_modelistation_supervisee_data_scientist.ipynb  # Notebook principal
â”œâ”€â”€ data/
â”‚   â””â”€â”€ 2016_Building_Energy_Benchmarking.csv   # Dataset brut
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ 2016_Building_Energy_Benchmarking_data_profile.html  # Profiling
â”‚   â”œâ”€â”€ memeo_variables.md                      # Documentation variables
â”‚   â””â”€â”€ note.md                                 # Notes projet
â””â”€â”€ env/                                        # Environnement virtuel (non versionnÃ©)
```

---

## ğŸ”§ Technologies UtilisÃ©es

| CatÃ©gorie | Librairies |
|-----------|------------|
| **Data Science** | pandas, numpy, scikit-learn |
| **ML AvancÃ©** | LightGBM |
| **Visualisation** | matplotlib, seaborn, plotly |
| **Statistiques** | statsmodels |
| **Notebook** | jupyter, ipykernel |

---

## ğŸ“Š Tableau RÃ©capitulatif Final

| Target | Meilleur ModÃ¨le | RÂ² Test | Algorithme | Raison |
|--------|----------------|---------|------------|--------|
| **Ã‰nergie** | Random Forest | **0.7479** | Bagging | Robuste aux outliers |
| **CO2** | LightGBM | **0.8967** | Boosting | Capte les interactions |

---

## ğŸ“ CompÃ©tences DÃ©montrÃ©es

âœ… Nettoyage et exploration de donnÃ©es (EDA)  
âœ… Feature Engineering crÃ©atif (10 nouvelles features)  
âœ… PrÃ©vention du data leakage  
âœ… ModÃ©lisation supervisÃ©e (4 algorithmes)  
âœ… Optimisation hyperparamÃ¨tres (GridSearchCV)  
âœ… Analyse de performance (RÂ², overfitting)  
âœ… Comparaison Bagging vs Boosting  
âœ… Visualisation interactive (Plotly)  
âœ… Documentation et reproductibilitÃ©

---

## ğŸš€ Applications Business

Ces modÃ¨les permettent de :

- PrÃ©dire la consommation Ã©nergÃ©tique et les Ã©missions CO2 de nouveaux bÃ¢timents
- Prioriser les actions d'efficacitÃ© Ã©nergÃ©tique selon les features importantes
- Estimer l'impact de rÃ©novations structurelles (surface, usage, sources d'Ã©nergie)
- Guider les politiques publiques de rÃ©duction des Ã©missions

---

## ğŸ‘¤ Auteur

**ClÃ©ment**  
Projet 3 - Parcours Data Scientist - OpenClassrooms  
DÃ©cembre 2025

---

## ğŸ“ Licence

Ce projet est rÃ©alisÃ© dans le cadre d'une formation OpenClassrooms.

---

## ğŸ†˜ Support

En cas de problÃ¨me :

1. VÃ©rifiez que Python 3.12+ est installÃ© : `python3 --version`
2. VÃ©rifiez que l'environnement est activÃ© : `which python` (doit pointer vers `env/bin/python`)
3. RÃ©installez les dÃ©pendances : `pip install -r requirements.txt`
4. RedÃ©marrez le kernel Jupyter

Pour toute question, consultez la documentation du notebook ou les commentaires dans le code.
