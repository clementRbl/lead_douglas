# üè¢ Pr√©diction de Consommation √ânerg√©tique et √âmissions CO2 des B√¢timents

> **Projet 3 - Parcours Data Scientist - OpenClassrooms**  
> Mod√©lisation supervis√©e pour la pr√©diction de la consommation √©nerg√©tique et des √©missions de gaz √† effet de serre des b√¢timents non-r√©sidentiels de Seattle (2016).

---

## üöÄ Installation et Lancement

### Pr√©requis

- Python 3.12 ou sup√©rieur
- Git

### √âtapes d'installation

```bash
# 1. Cloner le d√©p√¥t
git clone

# 2. Cr√©er un environnement virtuel
python3 -m venv env

# 3. Activer l'environnement
source env/bin/activate  # Linux/macOS
# OU
env\Scripts\activate     # Windows

# 4. Mettre √† jour pip
pip install --upgrade pip

# 5. Installer les d√©pendances
pip install -r requirements.txt

# 6. V√©rifier l'installation
pip list
```

### Lancer le notebook

```bash
# Ouvrir avec VS Code
code P3_template_modelistation_supervisee_data_scientist.ipynb

# OU avec Jupyter Notebook
jupyter notebook P3_template_modelistation_supervisee_data_scientist.ipynb
```

**‚ö†Ô∏è Important :** S√©lectionnez le kernel Python de l'environnement virtuel (`env/bin/python`) dans VS Code.

---

## üìä Description du Projet

### Objectifs

Construire **deux mod√®les de r√©gression supervis√©e** pour pr√©dire :

1. **La consommation √©nerg√©tique** (`SiteEnergyUse(kBtu)`)
2. **Les √©missions de CO2** (`TotalGHGEmissions`)

des b√¢timents non-r√©sidentiels de Seattle, √† partir de leurs caract√©ristiques structurelles et d'usage.

### Dataset

- **Source** : 2016 Building Energy Benchmarking
- **Taille initiale** : 3 376 b√¢timents √ó 46 colonnes
- **Taille finale** : 1 649 b√¢timents √ó 40 colonnes (apr√®s nettoyage)
- **P√©riode** : Ann√©e 2016

### Variables cl√©s

**Features principales :**
- `PropertyGFATotal` : Surface totale du b√¢timent (ft¬≤)
- `LargestPropertyUseTypeGFA` : Surface de l'usage principal
- `PrimaryPropertyType` : Type de b√¢timent (H√¥pital, Bureau, etc.)
- `YearBuilt` : Ann√©e de construction
- `NumberofBuildings` : Nombre de b√¢timents dans le complexe
- `NumberofFloors` : Nombre d'√©tages

**Variables cr√©√©es (Feature Engineering) :**
- `BuildingAge` : √Çge du b√¢timent
- `EnergyPerSurface` : Ratio √©nergie/surface (pr√©dicteur #1 pour CO2)
- `SurfaceGasInteraction` : Interaction surface √ó gaz naturel
- `HasNaturalGas`, `HasElectricity`, `HasSteam` : Indicateurs sources d'√©nergie
- `DistanceToCenter` : Distance au centre-ville (Haversine)

---

## üéØ R√©sultats Principaux

### Target 1 - Consommation √ânerg√©tique

| M√©trique | Valeur |
|----------|---------|
| **Meilleur mod√®le** | Random Forest (Bagging) - Optimis√© |
| **R¬≤ CV (Base)** | 0.7159 (72% variance expliqu√©e) |
| **R¬≤ CV (Optimis√©)** | **0.7288** (73% variance expliqu√©e) |
| **Am√©lioration** | **+1.8%** |
| **Top Features** | PropertyGFABuilding(s) (16.78%), PropertyGFATotal (13.71%), LargestPropertyUseTypeGFA (9.31%) |

**üí° Pourquoi Random Forest gagne ?**  
Plus robuste aux outliers gr√¢ce au **bagging** (parall√©lisation d'arbres). L'optimisation via GridSearchCV (144 combinaisons test√©es) a permis d'am√©liorer significativement les performances (+1.8%).

### Target 2 - √âmissions de CO2

| M√©trique | Valeur |
|----------|---------|
| **Meilleur mod√®le** | LightGBM (Boosting) - Optimis√© üöÄ |
| **R¬≤ CV (Base)** | 0.9214 (92% variance expliqu√©e) |
| **R¬≤ CV (Optimis√©)** | **0.9218** (92% variance expliqu√©e) |
| **Am√©lioration** | **+0.05%** |
| **Top Features** | SiteEnergyUse (16.97%), EnergyPerSurface (12.05%), DistanceToCenter (10.15%) |

**üí° Pourquoi LightGBM gagne ?**  
Capte mieux les **interactions complexes** entre features gr√¢ce au **boosting** s√©quentiel. L'optimisation via GridSearchCV (108 combinaisons test√©es) a confirm√© les hyperparam√®tres quasi-optimaux (+0.05%).

---

## üõ†Ô∏è M√©thodologie

### 1. Analyse Exploratoire (EDA)

- Nettoyage des donn√©es (suppression Multifamily, valeurs n√©gatives)
- Analyse des distributions (transformation log n√©cessaire)
- Matrice de corr√©lation (9 variables cl√©s)
- Visualisations interactives (Plotly)

### 2. Feature Engineering

- Cr√©ation de 10 nouvelles features
- Gestion du data leakage (exclusion EnergyPerSurface pour Target 1)
- Encodage OneHot des variables cat√©gorielles
- Imputation des valeurs manquantes

### 3. Mod√©lisation

**Algorithmes test√©s (4) :**
1. Linear Regression (baseline)
2. Random Forest (bagging) ‚≠ê
3. Support Vector Regressor (SVR)
4. LightGBM (boosting) ‚≠ê

**Architecture :**
- **Pipeline sklearn** : Encapsule preprocessing (ColumnTransformer) + mod√®le
- **Cross-Validation K-Fold** : K=5 splits pour √©valuation robuste
- **Avantage** : Pas de data leakage, √©valuation sur 5 validations diff√©rentes

**Optimisation :**
- **GridSearchCV avec Pipeline + CV** : Optimisation robuste des hyperparam√®tres
- Target 1 (Energy) : 144 combinaisons test√©es ‚Üí **+1.8% d'am√©lioration**
- Target 2 (CO2) : 108 combinaisons test√©es ‚Üí **+0.05% d'am√©lioration**

### 4. √âvaluation

- **M√©triques** : R¬≤ CV (mean¬±std), RMSE CV, MAE CV
- **Validation** : Cross-Validation K-Fold (K=5) - Pas de Train/Test unique
- **Feature Importance** : Extraction depuis mod√®les entra√Æn√©s
- **Analyse overfitting** : √âcart R¬≤ Train - R¬≤ CV (Overfit Gap)

---

## üìà Enseignements Cl√©s

1. **Les features d'interaction sont CRUCIALES**  
   `EnergyPerSurface` + `SurfaceGasInteraction` = 31.42% de l'importance pour CO2

2. **Bagging vs Boosting - Le contexte compte**  
   - **Random Forest** : Meilleur sur donn√©es avec outliers (Target 1)
   - **LightGBM** : Meilleur sur donn√©es avec interactions complexes (Target 2)

3. **La transformation log est essentielle**  
   Distribution asym√©trique ‚Üí log1p(target) am√©liore drastiquement les performances

4. **GridSearchCV avec Pipeline + CV am√©liore les performances**  
   L'approche robuste (Pipeline + Cross-Validation) permet une optimisation efficace (+1.8% et +0.05%)

5. **Les variables GFA (surface par usage) sont pr√©dictives**  
   `LargestPropertyUseTypeGFA` est le 2√®me pr√©dicteur le plus fort (+0.846 corr√©lation)

---

## üîß Technologies Utilis√©es

| Cat√©gorie | Librairies |
|-----------|------------|
| **Data Science** | pandas, numpy, scikit-learn |
| **ML Avanc√©** | LightGBM |
| **Visualisation** | matplotlib, seaborn, plotly |
| **Statistiques** | statsmodels |
| **Notebook** | jupyter, ipykernel |

---

## üìä Tableau R√©capitulatif Final

| Target      | Meilleur Mod√®le        | R¬≤ CV (Base) | R¬≤ CV (Optimis√©) | Am√©lioration | Algorithme |
| ----------- | ---------------------- | ------------ | ---------------- | ------------ | ---------- |
| **√ânergie** | Random Forest          | 0.7159       | **0.7288**       | **+1.8%**    | Bagging    |
| **CO2**     | LightGBM               | 0.9214       | **0.9218**       | **+0.05%**   | Boosting   |

**üí° Conclusion :** GridSearchCV avec Pipeline + Cross-Validation a permis d'optimiser les deux mod√®les de mani√®re robuste, √©vitant la suroptimisation sur un seul split de donn√©es.

---

## üéì Comp√©tences D√©montr√©es

‚úÖ Nettoyage et exploration de donn√©es (EDA)  
‚úÖ Feature Engineering cr√©atif (10 nouvelles features)  
‚úÖ Pr√©vention du data leakage  
‚úÖ Mod√©lisation supervis√©e (4 algorithmes)  
‚úÖ Optimisation hyperparam√®tres (GridSearchCV)  
‚úÖ Analyse de performance (R¬≤, overfitting)  
‚úÖ Comparaison Bagging vs Boosting  
‚úÖ Visualisation interactive (Plotly)  
‚úÖ Documentation et reproductibilit√©

---

## üöÄ Applications Business

Ces mod√®les permettent de :

- Pr√©dire la consommation √©nerg√©tique et les √©missions CO2 de nouveaux b√¢timents
- Prioriser les actions d'efficacit√© √©nerg√©tique selon les features importantes
- Estimer l'impact de r√©novations structurelles (surface, usage, sources d'√©nergie)
- Guider les politiques publiques de r√©duction des √©missions

---

## üÜò Support

En cas de probl√®me :

1. V√©rifiez que Python 3.12+ est install√© : `python3 --version`
2. V√©rifiez que l'environnement est activ√© : `which python` (doit pointer vers `env/bin/python`)
3. R√©installez les d√©pendances : `pip install -r requirements.txt`
4. Red√©marrez le kernel Jupyter

Pour toute question, consultez la documentation du notebook ou les commentaires dans le code.
